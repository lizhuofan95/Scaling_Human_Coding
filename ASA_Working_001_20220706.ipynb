{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ASA_Working_001_20220706.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lizhuofan95/Scaling_Human_Coding/blob/master/ASA_Working_001_20220706.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Workshop: Introduction to Machine Learning into Qualitative Research\n",
        "\n",
        "Coding allows us to see themes and patterns in qualitative data, but it can also get repetitive and dreary, especially with codes that are rather informational than interpretative. \n",
        "\n",
        "Previous approaches to automating qualitative coding rely heavily on prespecified rules, statistical assumptions, or dictionaries of keywords, often at the price of interpretative adaptability. \n",
        "\n",
        "Recent developments in deep learning models of natural languages provide a promising opportunity for qualitative researchers to scale their coding without compromising adaptability. Instead of following prespecified rules, deep learning algorithms are designed to mimic human behavior using human-generated examples. \n",
        "\n",
        "After iteratively establishing a codebook and some learning examples by hand-coding a sample of qualitative data, deep learning can help researchers quickly scale their initial codings on the remaining data and save up time and energy for thinking deeply about the data and the code. \n",
        "\n",
        "This Google Colab notebook walks you through the deep-learning-powered workflow we developed in our own study for analyzing in-depth interview data. "
      ],
      "metadata": {
        "id": "1Apsvt5Ouip3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Introduction to Python in Google Colab"
      ],
      "metadata": {
        "id": "qNSVZsvcvfM4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You are in Google Colab, or \"Colaboratory\", which allows you to write and execute Python in your browser without having to set up a programming environment on your own device. It also provides access to computational power for deep learning models free of charge for noncommercial uses. \n",
        "\n",
        "In this workshop, we use Google Colab and publicly available data to demonstrate the workflow, but you should NOT upload your own human subject data without a plan to protect their confidentiality! "
      ],
      "metadata": {
        "id": "a0YVTZQ1vh5R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now You can click the first sidebar button on the left to access the _table of Content_. Clicking \"cell(s) hidden\" will reveal more. You can also expand sections by clicking the small arrows to the left of headers. Try this now with the header \"practice running code.\"  "
      ],
      "metadata": {
        "id": "KnRIQUxp0Yq8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Running Python Code in Google Colab\n",
        "\n",
        "You can execute embedded code by clicking on the arrow to the left of the cell. This is how we run programs or use functions. Try this below. "
      ],
      "metadata": {
        "id": "vYsnXiLT0a6J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print (\"Hello world, I am here to code your qualitative data more efficiently!\")"
      ],
      "metadata": {
        "id": "e5Q8nKXX0fmv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now run the simple program below. It will ask for your name, and say if it likes it."
      ],
      "metadata": {
        "id": "EQqB3IAx0wk3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "name = input('What is your name? ')\n",
        "if name == 'Avery':\n",
        "  print('Awesome name!', name, 'is pretty cool.')\n",
        "elif name == \"Zhuofan\":\n",
        "  print('Good name!', name, 'is pretty cool.')\n",
        "else:\n",
        "  print(' Well', name, ', your name is good I guess.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kE6dQzIp0xGa",
        "outputId": "367d75c0-d276-4cd5-c40c-85c4dd0aa019"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "What is your name? Zhuofan\n",
            "Good name! Zhuofan is pretty cool.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that you understand the basics of this interface, the rest of this notebook will walk you through our deep-learning-powered workflow. "
      ],
      "metadata": {
        "id": "s8wYITdy5ARU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Overview of the Workflow\n",
        "\n",
        "(1) Developing Codebook V.1;\n",
        "\n",
        "(2) Coding Training Data;\n",
        "\n",
        "(3) **Importing data from ATLAS.ti**;\n",
        "\n",
        "(4) **Preprocessing**;\n",
        "\n",
        "(5) **Scaling**;\n",
        "\n",
        "(6) **Recoding**;\n",
        "\n",
        "(7) **Reimporting to ATLAS.ti**\n",
        "\n",
        "(8) Examing Machine-Coded Data and Revising Codebook;\n",
        "\n",
        "(9) Repeat (2)-(8) with Codebook V.2..."
      ],
      "metadata": {
        "id": "10Zo_i6v0zAN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Assuming that you have done an initial coding on a sample of the data, the following Python code does (3)-(7) for you. "
      ],
      "metadata": {
        "id": "2xeprVM24-8R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Part I: Importing data from ATLAS.ti\n",
        "\n",
        "Many qualitative researchers use QDA software to code their data, so here we start with an Excel spreadsheet that mimics the export function in ATLAS.ti. \n",
        "\n",
        "Assuming we code on the level of paragraphs, each row should contain all relevant information about a single paragraph. \n",
        "\n",
        "Column-wise, this spreadsheet should include the following columns:\n",
        "\n",
        "- Interview ID\n",
        "- Paragraph ID\n",
        "- Paragraph Content\n",
        "- Initial Codings\n",
        "\n",
        "Row-wise, it should include paragraphs that you have \"just coded\" as training data, some leave-out paragraphs"
      ],
      "metadata": {
        "id": "zl-C_rm26mzU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**\\[Insert a screen shot of the Excel export\\]**"
      ],
      "metadata": {
        "id": "P2AxF13d7990"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Importing Data from ATLAS.ti"
      ],
      "metadata": {
        "id": "OeNxG3c28g6a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads a python function for importing data from ATLAS.ti-generated Excel spreadsheets."
      ],
      "metadata": {
        "id": "nGs8qVHwAsJ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd  # The most commonly used library for data wrangling in Python\n",
        "\n",
        "class read_ATLAS:\n",
        "    \"\"\"Loads data ATLAS.ti-generated export files in .xls/.xlsx format\"\"\"\n",
        "    \n",
        "    def __init__(self, path):\n",
        "        \"\"\"\n",
        "        Reads excel files. \n",
        "\n",
        "        Args:\n",
        "            path: file path to an ATLAS export file. It must have one paragraph per row and for each paragraph \n",
        "                  include the following columns:\n",
        "\n",
        "                - Document: unique interview id\n",
        "                - Reference: unique pragraph id\n",
        "                - Quotation Content: paragraph content\n",
        "                - Codes: a list of codes that have been manually assigned to the paragraph\n",
        "        \"\"\"\n",
        "        \n",
        "        self.data = pd.read_excel(path)[['Document', 'Reference', 'Quotation Content', 'Codes']]\n",
        "    \n",
        "    def get_original(self):\n",
        "        \"\"\"\n",
        "        Reads the original version of the data. \n",
        "\n",
        "        Returns:\n",
        "            data: the original, abbreviated version of the data, which we will use for reimporting machine-generated codings back into ATLAS.ti.\n",
        "        \"\"\"\n",
        "        \n",
        "        data = self.data\n",
        "        \n",
        "        data['Codes'] = data['Codes'].apply(lambda x: str(x).split(\"\\n\"))\n",
        "        N = len(data)\n",
        "        \n",
        "        for i in range(N):\n",
        "            data['Codes'][i] = [data['Codes'][i][j].strip(\"#\") for j in range(len(data['Codes'][i]))]\n",
        "        \n",
        "        data['Codes_Frozen'] = data['Codes'].apply(frozenset).to_frame(name='Codes_Frozen')\n",
        "        for code in frozenset.union(*data.Codes_Frozen):\n",
        "            data[code] = data.apply(lambda _: int(code in _.Codes_Frozen), axis=1)\n",
        "        \n",
        "        return data\n",
        "    \n",
        "    def get_training(self, min_length = -1):\n",
        "        \"\"\"\n",
        "        Reads in the original data, but only keeps participants' speeches over a certain length for machine learning, \n",
        "        excluding researchers' notes, interviewers' speeches, and other functional text that are not being coded analytically. \n",
        "\n",
        "        This function also transforms the list of codes into one-hot encodings, such that the presence of any given code is \n",
        "        denoted by an entire columns of 0s and 1s. \n",
        "\n",
        "        Args:\n",
        "            min_length = the minimum number of characters that a paragraph must include to be included as valid data. This is to \n",
        "                         exclude filler sentences such as \"yeah\", \"no\", \"I know\" etc. Only comes into effect if you specify a \n",
        "                         value that is greater or equal to 1. \n",
        "\n",
        "        Returns: \n",
        "            data: the abbrevaited version of the data, which we will use for machine learning. \n",
        "        \"\"\"\n",
        "        data = self.data\n",
        "        \n",
        "        data['Quotation Content'] = data['Quotation Content'].apply(lambda x: str(x).split(\"\\t\"))\n",
        "        data['Codes'] = data['Codes'].apply(lambda x: str(x).split(\"\\n\"))\n",
        "        N = len(data)\n",
        "        data['Spk'] = \"\"\n",
        "        data['Quote'] = \"\"\n",
        "        for i in range(N):\n",
        "            line = data['Quotation Content'][i]\n",
        "            if len(line) >= 2:\n",
        "                data['Spk'][i] = line[0].strip(\":\\s\")\n",
        "                data['Quote'][i] = line[1].strip('\\u202c')\n",
        "            elif len(line) == 1:\n",
        "                data['Spk'][i] = \"\"\n",
        "                data['Quote'][i] = line[0].strip()\n",
        "            else:\n",
        "                data['Spk'][i] = data['Quote'] = \"\"\n",
        "            data['Codes'][i] = [data['Codes'][i][j].strip(\"#\") for j in range(len(data['Codes'][i]))]\n",
        "    \n",
        "        data['Codes_Frozen'] = data['Codes'].apply(frozenset).to_frame(name='Codes_Frozen')\n",
        "        for code in frozenset.union(*data.Codes_Frozen):\n",
        "            data[code] = data.apply(lambda _: int(code in _.Codes_Frozen), axis=1)\n",
        "    \n",
        "        #data = data[data[\"Spk\"].isin([\"MSPKR\",\"FSPKR\"])].reset_index(drop = True)\n",
        "        data = data[(data[\"Spk\"] != \"\") & (data[\"INT_NEW\"] != 1)].reset_index(drop = True)\n",
        "        \n",
        "        if min_length >= 0: \n",
        "            data = data[data['Quote'].map(len) >= min_length].reset_index(drop = True)\n",
        "        \n",
        "        cols = [col for col in data.columns if col not in ['Quotation Content', 'Codes', 'Spk', 'Codes_Frozen']]\n",
        "    \n",
        "        return data[cols]"
      ],
      "metadata": {
        "id": "qDQY5Zqi79XO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell load the actual Excel file from our GitHub repository and executives the function above to import data into Python. "
      ],
      "metadata": {
        "id": "S-MucRfYBL39"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "path = \"data\\July_v3_INT_NEWisquestion.xlsx\"\n",
        "\n",
        "data_original = read_ATLAS(path = path).get_original()\n",
        "\n",
        "data_ML = read_ATLAS(path = path).get_training(min_length = 10)"
      ],
      "metadata": {
        "id": "SCGomGqM8QYb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how our original data should look like in Python:"
      ],
      "metadata": {
        "id": "bWt7Pd6u79qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_original"
      ],
      "metadata": {
        "id": "BIzQtoPeBw_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how our data should look like in one-hot encoding, which we will use for machine learning:"
      ],
      "metadata": {
        "id": "7h-M0YxNB4Va"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_ML"
      ],
      "metadata": {
        "id": "R59itba5B754"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing"
      ],
      "metadata": {
        "id": "GkV4vMDpCKi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell loads a function that translates our raw text data into something computer can understand - numerical vectors. We use Bidirectional Encoder Representation from Transformers, or BERT, a deep learning language model often trained on enourmous volumes of text (for English, 3300 million words from books and wikipedia articles) to represent fine-grained sequential and contextual information at all levels of natural language in high-dimensional vectors. \n",
        "\n",
        "Our implementation uses `PyTorch`, a popular deep learning library developed by Facebook and pretrained BERT models available through the `Transformers` library by another company called HuggingFace. "
      ],
      "metadata": {
        "id": "b92lKgGYCTF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from transformers import DistilBertModel, DistilBertTokenizer\n",
        "\n",
        "def text2feature(text, MAX_LENGTH = 512, BATCH_SIZE = 16, device = \"cuda\"):\n",
        "    \"\"\"\n",
        "    Use pretrained BERT model to vectorize raw text. \n",
        "    \n",
        "    Args:\n",
        "        text: the text of interest, stored in the \"Quote\" column of our DataFrame \"data_ML\". \n",
        "        \n",
        "        MAX_LENGTH: the maximum number of \"wordpieces\" (usually words but not always) in each paragraph to be used \n",
        "                    for representing the paragraph, capped at 512. \n",
        "        \n",
        "        BATCH_SIZE: the maximum number of samples to be used in a single neural network iteration. It is recommended\n",
        "                    to use a batch size of 32 or 64. We used 16 due to the limitation of our GPU memory. \n",
        "        \n",
        "        device: \"CPU\" or \"cuda\". \"cuda\" is the architecture of the NVIDIA graphics processing unit (GPU) which is\n",
        "                used to accelerate machine learning. Only use if you either (1) have a NVIDIA GPU and have installed \n",
        "                the CUDA development tools following the instruction or (2) use cloud computing (e.g. Google Colab). \n",
        "        \n",
        "    Returns: \n",
        "        feature: an N_row by 768 array of vectors that represent each paragraph using a vector of 768.\n",
        "    \"\"\"\n",
        "\n",
        "    tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
        "    \n",
        "    encodings = list(map(lambda t: tokenizer.encode(t, padding=True, truncation=True, max_length = MAX_LENGTH, add_special_tokens=True), text))\n",
        "    \n",
        "    max_len = 0\n",
        "    for i in encodings:\n",
        "        if len(i) > max_len:\n",
        "            max_len = len(i)\n",
        "\n",
        "    encodings_padded = np.array([i + [0]*(max_len-len(i)) for i in encodings])\n",
        "    attention_mask = [[float(i > 0) for i in ii] for ii in encodings_padded]\n",
        "    \n",
        "    dataset = TensorDataset(torch.tensor(encodings_padded, dtype = torch.int), torch.tensor(attention_mask))\n",
        "    \n",
        "    dataloader = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "    \n",
        "    model = DistilBertModel.from_pretrained('distilbert-base-uncased').to(device)\n",
        "    \n",
        "    features = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for step_num, batch_data in enumerate(dataloader):\n",
        "            token_ids, masks = tuple(t.to(device) for t in batch_data)\n",
        "            last_hidden_states = model(token_ids, masks)\n",
        "            features.append(last_hidden_states[0][:,0,:].cpu().detach().numpy())\n",
        "            \"\"\"\n",
        "            The model actually produces a vector of 768 for each of the 512 \"wordpiece\" in every sequence, but the \n",
        "            vector of every sequence, denoted by [CLS], is always a special classification token that can be used as\n",
        "            the aggregate sequence representation for classification tasks. An alterantive is to represent the sequence\n",
        "            by averaging all 512 vectors, which tends to produce similar results.           \n",
        "            \"\"\"\n",
        "    features = np.vstack(features)\n",
        "    \n",
        "    return features"
      ],
      "metadata": {
        "id": "7cpFbVN-CCxS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell executes the function above and transforms text data into vectors. We should obtain a N Ã— 768 matrix that represents each row using a 768-dimensional vector. "
      ],
      "metadata": {
        "id": "IsGy7FbbDEFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = data_ML['Quote'].values.tolist()\n",
        "features = text2feature(text)\n",
        "features.shape"
      ],
      "metadata": {
        "id": "QcyBCSceCC55"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Our data look like this in vectors:"
      ],
      "metadata": {
        "id": "7uzUb9zrDKAi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "features"
      ],
      "metadata": {
        "id": "kwqJz37LCC8X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "But before we extending our initial codings to all the remaining data, we want to have an sense of how reliable it would be. We do so by setting aside a small sample of coded data and compare machine-generated codings against our own codings on this already coded sample. \n",
        "\n",
        "To make sure the accuracy of our predictions is not dependent upon the idiosyncracy of the training/test data used, we randomly split our coded cases into training and test data and split multiple times to ensure that our results are not dependent upon . We want as much training data as possible, but we also want to reserve about 15%-30% for testing. "
      ],
      "metadata": {
        "id": "kpCR8nBmEpqC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "def split(all_cases, n_splits = 10, test_size = 0.25):\n",
        "    \"\"\"\n",
        "    Split any coded data into training and test sets. \n",
        "    \n",
        "    Args:\n",
        "        n_splits: how many randomly reshuffled splits to generate. The default is 10. \n",
        "        \n",
        "        test_size: how many cases to use as test data. \n",
        "                   - If between 0 and 1, represents the proportion of the dataset to include;\n",
        "                   - If integer greater than 1, represents the the absolute number of test samples.\n",
        "                   - The default is 25%. \n",
        "    Returns:\n",
        "        train_set, test_set\n",
        "    \n",
        "    \"\"\"\n",
        "    train_test_split = ShuffleSplit(n_splits = n_splits, test_size = test_size)\n",
        "    train_set = []\n",
        "    test_set = []\n",
        "    for train_index, test_index in train_test_split.split(all_cases):\n",
        "        train_set.append(train_index.tolist())\n",
        "        test_set.append(test_index.tolist())\n",
        "    \n",
        "    return train_set, test_set"
      ],
      "metadata": {
        "id": "8GIKgORtElY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell specifies the number of random shuffle-splits and the number of interviews to reserve for testing:"
      ],
      "metadata": {
        "id": "k5n2IW4OFYIG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_splits = 20\n",
        "test_size = 4"
      ],
      "metadata": {
        "id": "AjhDowwIFhw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell specifies a list of interviews to use for training/testing (from the \"Document\" column) and a list of codes to scale (from the \"Codes\" column):"
      ],
      "metadata": {
        "id": "iTPRvraPFoMk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_cases = ['7069_CASE', \n",
        "             '4039_CASE', \n",
        "             '7061_CASE', \n",
        "             '7522_CASE', \n",
        "             '7068_CASE', \n",
        "             '7044_CASE', \n",
        "             '7049_CASE', \n",
        "             '10018_CASE', \n",
        "             '10002_CASE', \n",
        "             '10021_CASE', \n",
        "             '7035_CASE',\n",
        "             '7028_CASE',\n",
        "             '7512_CASE',\n",
        "             '7031_CASE'\n",
        "            ]\n",
        "\n",
        "eval_codes = ['IllnessNarrative', 'SOURCESofCULTURE_FamilyOrCommunity', 'SOURCESofCULTURE_Medicine', 'HealthcareSystemsIssues']"
      ],
      "metadata": {
        "id": "Mg-CVpaTFoWT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell executes the shuffle-split and generates the training and testing datasets:"
      ],
      "metadata": {
        "id": "Qm78CKz9GjYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_set, test_set = split(all_cases, n_splits, test_size)"
      ],
      "metadata": {
        "id": "8fSW2qo3Giu6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Scaling"
      ],
      "metadata": {
        "id": "aW5wKZ_oGrPS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since BERT is typically deployed with data that are much bigger and deep learning models much more sophisticated than required by our tasks, here we simplify it by combining BERT vectors with a regularized logistic regression, which despite the strong assumption of linearity has been proven highly efficient for smaller scale data. "
      ],
      "metadata": {
        "id": "PzD16qYKG0Zq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score, cohen_kappa_score\n",
        "\n",
        "class Classifier:\n",
        "    def __init__(self):\n",
        "        \n",
        "        self.clf = LogisticRegression(penalty = 'l2', solver = 'liblinear', C = 1, class_weight= 'balanced')\n",
        "        \n",
        "    def train(self, features: NDArray, labels: NDArray):\n",
        "\n",
        "        self.clf.fit(features, labels)\n",
        "    \n",
        "    def predict(self, features: NDArray) -> NDArray:\n",
        "\n",
        "        predictions = self.clf.predict(features)\n",
        "        predprob = self.clf.predict_proba(features)\n",
        "    \n",
        "        return predictions, predprob\n",
        "    \n",
        "    def code(self, train_features, test_features, train_labels):\n",
        "        \n",
        "        self.train(train_features, train_labels)\n",
        "        self.predicted_labels, self.predicted_probabilities = self.predict(test_features)\n",
        "        \n",
        "        return self.predicted_labels, self.predicted_probabilities\n",
        "    \n",
        "    def metrics(self, test_labels):\n",
        "        \n",
        "        accuracy = accuracy_score(test_labels, self.predicted_labels)\n",
        "        f1 = f1_score(test_labels, self.predicted_labels, pos_label=1)\n",
        "        precision = precision_score(test_labels, self.predicted_labels)\n",
        "        recall = recall_score(test_labels, self.predicted_labels)\n",
        "        kappa = cohen_kappa_score(test_labels, self.predicted_labels)\n",
        "        \n",
        "        return [accuracy, f1, precision, recall, kappa]\n",
        "\n",
        "def classify(classifier, data, eval_codes, train_set, test_set, method = ['pred']):\n",
        "    \"\"\"\n",
        "    Train a machine learning model to predict human codings of interest based on each training/test split.\n",
        "    \n",
        "    Args:\n",
        "        classifier: the machine learning model. \n",
        "        \n",
        "        eval_codes: the list of codes to be scaled, corresponding to the \"Codes\" column from the ATLAS.ti export file.\n",
        "        \n",
        "        train_set/test_set: the ids of interviews to use as training/test data. \n",
        "        \n",
        "        method: \"pred\" or \"eval\". \n",
        "                \n",
        "                - The \"pred\" method predicts codings on uncoded data without returning metrics. \n",
        "                - The \"eval\" method predicts codings on coded data and compare the predictions against original human codings. \n",
        "        \n",
        "    Returns:\n",
        "        predictions: the predicted probability of a code on a paragraph.\n",
        "        \n",
        "        metrics: a list of performance metrics the \"eval\" method generates. \n",
        "    \n",
        "    \"\"\"\n",
        "    \n",
        "    predictions = pd.DataFrame({'code': [], 'test_set': [], 'p': []})\n",
        "    metrics = pd.DataFrame({'code': [], 'test_set': [], 'n':[], 'accuracy': [], 'f1': [], 'precision': [], 'recall': [],  \n",
        "                            'kappa': []})\n",
        "    \n",
        "    for code in eval_codes:\n",
        "        for train_ids, test_ids in zip(train_set, test_set):\n",
        "\n",
        "            train_data = data[data['Document'].isin([all_cases[id] for id in train_ids])]\n",
        "            test_data = data[-data['Document'].isin([all_cases[id] for id in train_ids])]\n",
        "\n",
        "            train_id = train_data.index.tolist()\n",
        "            test_id = test_data.index.tolist()\n",
        "\n",
        "            train_features = features[train_id]\n",
        "            test_features = features[test_id]\n",
        "\n",
        "            train_labels = train_data[code].values.tolist()\n",
        "            \n",
        "            predicted_labels, predicted_probabilities = classifier.code(train_features, test_features, train_labels)\n",
        "\n",
        "            predictions = predictions.append(pd.Series([code, test_ids] + [[x[1] for x in predicted_probabilities]], index = predictions.columns), ignore_index=True)\n",
        "\n",
        "            if method == 'eval':\n",
        "                test_labels = test_data[code].values.tolist()\n",
        "                metrics = metrics.append(pd.Series([code, test_ids, data_ML.loc[data_ML['Document'].isin([all_cases[id] for id in train_ids]), code].sum()] + classifier.metrics(test_labels), index = metrics.columns), ignore_index=True) \n",
        "            \n",
        "    return predictions, metrics"
      ],
      "metadata": {
        "id": "GAY_6iiiGxD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell executes the machine learning model and evaluates the results against test data: "
      ],
      "metadata": {
        "id": "8dxGrFQQJcIq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions, metrics = classify(Classifier(), data_ML, eval_codes, train_set, test_set, 'eval')"
      ],
      "metadata": {
        "id": "MeiMrtcUJP-u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pd.DataFrame({'N': data_ML[eval_codes].sum()}).join(metrics.groupby('code')[['n', 'accuracy', 'f1', 'precision', 'recall', 'kappa']].mean())"
      ],
      "metadata": {
        "id": "UMgStuMaJrdl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.sort_values(['recall', 'f1'], ascending = False).groupby('code').nth(0)"
      ],
      "metadata": {
        "id": "Kq1KAxC1JzeA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "XacsjfS6J1Lc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_set = [[0, 1, 2, 3, 4, 5, 6, 7]]\n",
        "test_set = [[8, 9, 10]]\n",
        "\n",
        "predictions, _ = classify(Classifier(), data_ML, eval_codes, train_set, test_set)"
      ],
      "metadata": {
        "id": "aLk0sLZ4J1S_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Recoding\n",
        "\n",
        "Now that we have casted a wide net that presumbly catches about 70% of all paragraphs that would have been coded as \"Educational Background\", we want to quickly review the results and filter out obivous false positives.\n",
        "\n",
        "This cell creates "
      ],
      "metadata": {
        "id": "e4GLK0MSLgqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "recode_set = [8, 9, 10]\n",
        "combine_codes = [0, 1, 2, 3]\n",
        "\n",
        "sorting = [False, False, False, False]\n",
        "sorting_vars = [['IllnessNarrative'], ['HealthcareSystemsIssues']]\n",
        "\n",
        "pruning = [True, True, True, True]\n",
        "pruning_thresholds = [0.5, 0.5, 0.5, 0.5]"
      ],
      "metadata": {
        "id": "A9QbbqrfLeJm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_recode(data, predictions, all_cases, recode_set, combine_codes, sorting, pruning, sorting_vars = None, pruning_threshold = None):\n",
        "    \n",
        "    recode = data[data['Document'].isin([all_cases[id] for id in recode_set])][['Quote', 'Document', 'Reference']].reset_index(drop = True)\n",
        "    \n",
        "    for code in eval_codes:\n",
        "        #recode_set = best.loc[code]['test_set']\n",
        "        p = predictions.loc[(predictions['code'] == code) & (predictions['test_set'].apply(lambda x: x==recode_set))]['p'].values.tolist()[0]\n",
        "        col = pd.DataFrame({str(combine_codes[eval_codes.index(code)]) + '_P_' + code :p})\n",
        "        recode = recode.join(col)\n",
        "    \n",
        "    for doc_id in range(max(combine_codes)+1):\n",
        "        out = recode[['Document', 'Reference']]\n",
        "        name = \"Pred_\"\n",
        "\n",
        "        rowids_to_keep = []\n",
        "\n",
        "        for col in (col for col in recode if col.startswith(str(doc_id))):\n",
        "\n",
        "            out = out.join(recode[[col]])\n",
        "\n",
        "            colname = col.split('_',1)[1]\n",
        "\n",
        "            out.rename(columns={col: colname}, inplace=True)\n",
        "\n",
        "            out[colname] = ''\n",
        "\n",
        "            name = name + colname + \"_\"\n",
        "\n",
        "            if pruning[doc_id] == True:\n",
        "                rowids_to_keep.append(out[colname].index[out[colname] >= pruning_threshold[doc_id]].tolist() )\n",
        "\n",
        "        #if sorting[doc_id] == True:\n",
        "        #    out.sort_values(by = [\"P_\" + v for v in sorting_vars[doc_id]], inplace = True, ascending = False)\n",
        "\n",
        "        out = out.join(recode[['Quote']])\n",
        "\n",
        "        if pruning[doc_id] == True:\n",
        "            out.iloc[list(set([item for sublist in rowids_to_keep for item in sublist]))].sort_index().to_excel(name + \".xlsx\")\n",
        "        else:\n",
        "            out.to_excel(\"recoding\\\" + name + \".xlsx\")"
      ],
      "metadata": {
        "id": "2pxLh1_gMhJX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "gen_recode(data_ML, predictions, all_cases, recode_set, combine_codes, sorting, pruning, sorting_vars, pruning_thresholds)"
      ],
      "metadata": {
        "id": "YYrrPzdhMhf2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "recoded_set = [\"IllnessNarrative\", \"SOURCESofCULTURE_FamilyOrCommunity\"]\n",
        "\n",
        "recoded = pd.read_excel(\"recoding\\Recoded_Pred_P_IllnessNarrative_P_SOURCESofCULTURE_FamilyOrCommunity_P_SOURCESofCULTURE_Medicine_.xlsx\")"
      ],
      "metadata": {
        "id": "W0NlBNczMjZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def recoding_metrics(data, recoded, recoded_set):\n",
        "\n",
        "    metrics = pd.DataFrame({'code': [], 'accuracy': [], 'f1': [], 'precision': [], 'recall': [], 'alpha': [], \n",
        "                            'kappa': []})\n",
        "    \n",
        "    for code in recoded_set:\n",
        "        recoded.loc[recoded[code].isna(), code] = recoded.loc[recoded[code].isna(), \"P_\"+code]\n",
        "        \n",
        "        predicted_labels = data.loc[(data['Document'].isin([all_cases[id] for id in recode_set])), code].values.tolist()\n",
        "        test_labels = [(x>=0.5)*1 for x in recoded[code].values.tolist()]\n",
        "    \n",
        "        accuracy = accuracy_score(test_labels, predicted_labels)\n",
        "        f1 = f1_score(test_labels, predicted_labels, pos_label=1)\n",
        "        precision = precision_score(test_labels, predicted_labels)\n",
        "        recall = recall_score(test_labels, predicted_labels)\n",
        "        alpha = krippendorff.alpha(np.stack((test_labels, predicted_labels)))\n",
        "        kappa = cohen_kappa_score(test_labels, predicted_labels)\n",
        "        \n",
        "        metrics = metrics.append(pd.Series([code, accuracy, f1, precision, recall, alpha, kappa], index = metrics.columns), ignore_index=True) \n",
        "    \n",
        "    return metrics"
      ],
      "metadata": {
        "id": "PA7Yr9Z3MqNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "metrics_x = recoding_metrics(data_ML, recoded, recoded_set)\n",
        "\n",
        "metrics_x.to_excel(\"output\\Recoding_metrics_all.xlsx\")"
      ],
      "metadata": {
        "id": "RnFe4JqlMqqG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Reimporting to ATLAS.ti"
      ],
      "metadata": {
        "id": "ZtKZ7oPdMy7O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def reimport2ATLAS(full_data, recoded, recoded_set, recoded_cases):\n",
        "    for code in recoded_set:\n",
        "        recoded.loc[recoded[code].isna(), code] = recoded.loc[recoded[code].isna(), \"P_\"+code]\n",
        "        recoded[code] = recoded[code].apply(lambda x: x >= 0.5).astype(int)\n",
        "        full_data.merge(recoded[['Document', 'Reference', code]], on = ['Document', 'Reference'], how = 'left')\n",
        "    \n",
        "    code_list = [col for col in full_data.columns.tolist() if col not in ['Document', 'Reference', 'Quote', 'Quotation Content', 'Codes', 'Codes_Frozen','nan']]\n",
        "    \n",
        "    for i in range(len(full_data)):\n",
        "        if full_data.loc[i,\"Document\"] in all_cases:\n",
        "            for code in code_list:\n",
        "                if(full_data.loc[i, code] >= 0.5): full_data.loc[i,'Quotation Content'] = full_data.loc[i,'Quotation Content'] + ' ' + '#' + code\n",
        "        \n",
        "    full_data['idx'] = full_data.groupby('Document').cumcount()\n",
        "    full_data['p_idx'] = 'p' + full_data['idx'].astype(str)\n",
        "    full_pivot = full_data.pivot(index='Document',columns='p_idx', values = 'Quotation Content')\n",
        "    full_pivot = full_pivot.reindex(sorted(full_pivot.columns, key=lambda x: float(x[1:])), axis=1)\n",
        "    full_pivot.to_excel(\"output\\Recoded_output_transformed_all_cases_all_codes.xlsx\")"
      ],
      "metadata": {
        "id": "5NtigcqjMs5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reimport2ATLAS(data_original, recoded, recoded_set, all_cases)"
      ],
      "metadata": {
        "id": "-785XnDKMuP_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "lIyXowWDLaxp"
      }
    }
  ]
}